{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Github](https://github.com/shachar700/Graph-Embedding/)"
      ],
      "metadata": {
        "id": "E9Cy-3sA2L46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Install python libraries\"\"\"\n",
        "!pip install gensim networkx scikit-learn wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Riu8JWi7T3Vz",
        "outputId": "52eba3f7-883a-4763-a0a2-e862c118dcf3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=9da102ae46f63bb66023cce62400ef38fb35032ae9682e2b79acff44f5e36cc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "content_url = \"https://raw.githubusercontent.com/shachar700/Graph-Embedding/refs/heads/master/data/cora/cora.content\"\n",
        "cites_url = \"https://raw.githubusercontent.com/shachar700/Graph-Embedding/refs/heads/master/data/cora/cora.cites\"\n",
        "\n",
        "def download_file(url, file_name):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"Downloaded: {file_name}\")\n",
        "\n",
        "download_file(content_url, \"cora.content\")\n",
        "download_file(cites_url, \"cora.cites\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DewDr3J-u9xE",
        "outputId": "bb09d0a7-2eac-4fec-fbcd-7a6544459be4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: cora.content\n",
            "Downloaded: cora.cites\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data to Google colab\n",
        "#Upload or drag and drop to 'Files' tab the following files:\n",
        "#cora.cites, cora.content\"\"\"\n",
        "\n",
        "\"\"\"from google.colab import files\n",
        "uploaded = files.upload()\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W2TshyZVTJJY",
        "outputId": "259e621e-d46b-4be7-9939-a86b3f10733d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from google.colab import files\\nuploaded = files.upload()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bceih7d2OVKg",
        "outputId": "ecca1840-e0af-48cc-ae55-130c07273036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deepwalk:\n",
            "0.647\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon May  7 21:25:07 2018\n",
        "\n",
        "@author: dedekinds\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue May  8 08:47:08 2018\n",
        "\n",
        "@author: dedekinds\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "directed = True\n",
        "p = 1.0#对于node2vec中的p==q时候等价于deepwwalk   #For p==q in node2vec, it is equivalent to deepwwalk\n",
        "q = 1.0\n",
        "num_walks = 1000\n",
        "walk_length = 100\n",
        "emb_size = 200\n",
        "iteration = 5\n",
        "\n",
        "\n",
        "LABEL = {\n",
        "      'Case_Based':1,\n",
        "\t\t'Genetic_Algorithms':2,\n",
        "\t\t'Neural_Networks':3,\n",
        "\t\t'Probabilistic_Methods':4,\n",
        "\t\t'Reinforcement_Learning':5,\n",
        "\t\t'Rule_Learning':6,\n",
        "\t\t'Theory':7\n",
        "        }\n",
        "'''\n",
        "得到Cora数据集中paper的ID和对应的分类label\n",
        "Get the ID and corresponding classification label of the paper in the Cora dataset\n",
        "'''\n",
        "def load_features(filename):\n",
        "    ids, labels = [], []\n",
        "    with open(filename, 'r') as f:\n",
        "        line = f.readline();\n",
        "        while line:\n",
        "            line_split = line.split();\n",
        "\n",
        "            ids.append(line_split[0]);\n",
        "            labels.append(line_split[-1]);\n",
        "            line = f.readline();\n",
        "\n",
        "        return ids, labels\n",
        "\n",
        "\n",
        "'''\n",
        "根据互引用的关系构造有向图\n",
        "Constructing a directed graph based on mutual reference relationships\n",
        "'''\n",
        "def load_graph(filename, id_list):\n",
        "    if directed:\n",
        "        g = nx.DiGraph()\n",
        "    else:\n",
        "        g = nx.Graph()\n",
        "    with open(filename, 'r') as f:\n",
        "        line = f.readline()\n",
        "        while line:\n",
        "            line_split = line.split()\n",
        "            #print(line_split)\n",
        "\n",
        "            if line_split[0] in id_list and line_split[1] in id_list  and line_split[0] != line_split[1]:\n",
        "                g.add_edge(line_split[0], line_split[1])\n",
        "                g[line_split[0]][line_split[1]]['weight'] = 1\n",
        "\n",
        "            line = f.readline()\n",
        "    return g\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_transition_probs(g, directed = False, p=1, q=1):\n",
        "    alias_nodes, alias_edges = {}, {};\n",
        "    for node in g.nodes():\n",
        "        probs = [g[node][nei]['weight'] for nei in sorted(g.neighbors(node))]\n",
        "        norm_const = sum(probs)\n",
        "        norm_probs = [float(prob)/norm_const for prob in probs]\n",
        "        alias_nodes[node] = get_alias_nodes(norm_probs)\n",
        "\n",
        "    if directed:\n",
        "        for edge in g.edges():\n",
        "            alias_edges[edge] = get_alias_edges(g, edge[0], edge[1], p, q)\n",
        "            #print(alias_edges[edge])\n",
        "    else:\n",
        "        for edge in g.edges():\n",
        "            alias_edges[edge] = get_alias_edges(g, edge[0], edge[1], p, q)\n",
        "            alias_edges[(edge[1], edge[0])] = get_alias_edges(g, edge[1], edge[0], p, q)\n",
        "\n",
        "    return alias_nodes, alias_edges\n",
        "\n",
        "\n",
        "def get_alias_edges(g, src, dest, p=1, q=1):\n",
        "    probs = [];\n",
        "    for nei in sorted(g.neighbors(dest)):\n",
        "        if nei==src:\n",
        "            probs.append(g[dest][nei]['weight']/p)\n",
        "        elif g.has_edge(nei, src):\n",
        "            probs.append(g[dest][nei]['weight'])\n",
        "        else:\n",
        "            probs.append(g[dest][nei]['weight']/q)\n",
        "    norm_probs = [float(prob)/sum(probs) for prob in probs]\n",
        "    return get_alias_nodes(norm_probs)\n",
        "\n",
        "'''\n",
        "针对节点t来说，我们得到了t能转移到不同类别节点的概率，\n",
        "常规做法是归一化之后按照概率随机选取，但这篇论文并没有直接这样做，而是选用了Alias算法进行抽样\n",
        "For node t, we get the probability that t can be transferred to different categories of nodes.\n",
        "The conventional approach is to normalize and then randomly select according to the probability,\n",
        "but this paper does not do this directly, but uses the Alias ​​algorithm for sampling\n",
        "'''\n",
        "def get_alias_nodes(probs):\n",
        "    l = len(probs)\n",
        "    a, b = np.zeros(l), np.zeros(l, dtype=np.int64)\n",
        "    small, large = [], []\n",
        "\n",
        "    for i, prob in enumerate(probs):\n",
        "        a[i] = l*prob\n",
        "        if a[i]<1.0:\n",
        "            small.append(i)\n",
        "        else:\n",
        "            large.append(i)\n",
        "\n",
        "    while small and large:\n",
        "        sma, lar = small.pop(), large.pop()\n",
        "        b[sma] = lar\n",
        "        a[lar]+=a[sma]-1.0\n",
        "        if a[lar]<1.0:\n",
        "            small.append(lar)\n",
        "        else:\n",
        "            large.append(lar)\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def node2vec_walk(g, start, alias_nodes, alias_edges, walk_length=30):\n",
        "    path = [start]\n",
        "    while len(path)<walk_length:\n",
        "        node = path[-1]\n",
        "        neis = sorted(g.neighbors(node))\n",
        "        if len(neis)>0:\n",
        "            if len(path)==1:\n",
        "                l = len(alias_nodes[node][0])\n",
        "                idx = int(np.floor(np.random.rand()*l))\n",
        "                if np.random.rand()<alias_nodes[node][1][idx]:\n",
        "                    path.append(neis[idx])\n",
        "                else:\n",
        "                    path.append(neis[alias_nodes[node][0][idx]])\n",
        "            else:\n",
        "                prev = path[-2]\n",
        "                l = len(alias_edges[(prev, node)][0])\n",
        "                idx = int(np.floor(np.random.rand()*l))\n",
        "                if np.random.rand()<alias_edges[(prev, node)][1][idx]:\n",
        "                    path.append(neis[idx])\n",
        "                else:\n",
        "                    path.append(neis[alias_edges[(prev, node)][0][idx]])\n",
        "        else:\n",
        "            break\n",
        "    return path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "edge_path = 'cora.content'\n",
        "label_path = 'cora.cites'\n",
        "model_path = './output_deepwalk.models'\n",
        "\n",
        "# load feature and adjacent matrix from file\n",
        "id_list, labels = load_features(edge_path)\n",
        "g = load_graph(label_path, id_list)#print(g)\n",
        "\n",
        "\n",
        "# s7 - adding islands to the graph\n",
        "for node in id_list:\n",
        "    if not g.has_node(node):\n",
        "        g.add_node(node)\n",
        "\n",
        "\n",
        "if os.path.isfile(model_path):\n",
        "    model = Word2Vec.load(model_path)\n",
        "    print ('load model successfully')\n",
        "else:\n",
        "    alias_nodes, alias_edges = preprocess_transition_probs(g, directed,p,q)\n",
        "\n",
        "\n",
        "    walks = []\n",
        "    idx_total = []\n",
        "    for i in range(num_walks):\n",
        "        r = np.array(range(len(id_list)))\n",
        "        np.random.shuffle(r)\n",
        "        #r = list(r)\n",
        "        #idx_total+=r\n",
        "        for node in [id_list[j] for j in r]:\n",
        "            walks.append(node2vec_walk(g, node, alias_nodes, alias_edges, walk_length))\n",
        "\n",
        "    model = Word2Vec(walks, vector_size=emb_size, min_count=0, sg=1, epochs=iteration)\n",
        "    model.save('output_deepwalk.model')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y=[]\n",
        "for temp in range(2708):\n",
        "    y.append( LABEL[labels[temp]])\n",
        "y = np.array(y)\n",
        "\n",
        "x_train = np.zeros(emb_size)\n",
        "x_test =  np.zeros(emb_size)\n",
        "\n",
        "\n",
        "droppoint = 500\n",
        "\n",
        "for x in range(droppoint):\n",
        "    x_train = np.row_stack((x_train,model.wv[id_list[x]]))\n",
        "x_train = np.delete(x_train,[0],axis = 0)\n",
        "y_train = y[:droppoint]\n",
        "\n",
        "\n",
        "for x in range(droppoint,1500):\n",
        "    x_test = np.row_stack((x_test,model.wv[id_list[x]]))\n",
        "x_test = np.delete(x_test,[0],axis = 0)\n",
        "y_test = y[droppoint:1500]\n",
        "\n",
        "#\n",
        "#neigh = ExtraTreesClassifier()\n",
        "#neigh.fit(x_train, y_train)\n",
        "#preds = neigh.predict(x_test)\n",
        "#print (list(preds-y_test).count(0)/500)\n",
        "\n",
        "\n",
        "classifier=LogisticRegression()\n",
        "classifier.fit(x_train,y_train)\n",
        "predictions=classifier.predict(x_test)\n",
        "print ('deepwalk:')\n",
        "print (list(predictions-y_test).count(0)/1000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon May  7 21:25:07 2018\n",
        "\n",
        "@author: dedekinds\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue May  8 08:47:08 2018\n",
        "\n",
        "@author: dedekinds\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "directed = True\n",
        "p = 5.0#对于node2vec中的p==q时候等价于deepwwalk\n",
        "q = 1.0\n",
        "num_walks = 1000\n",
        "walk_length = 100\n",
        "emb_size = 200\n",
        "iteration = 5\n",
        "\n",
        "\n",
        "LABEL = {\n",
        "        'Case_Based':1,\n",
        "\t\t'Genetic_Algorithms':2,\n",
        "\t\t'Neural_Networks':3,\n",
        "\t\t'Probabilistic_Methods':4,\n",
        "\t\t'Reinforcement_Learning':5,\n",
        "\t\t'Rule_Learning':6,\n",
        "\t\t'Theory':7\n",
        "        }\n",
        "'''\n",
        "得到Cora数据集中paper的ID和对应的分类label\n",
        "'''\n",
        "def load_features(filename):\n",
        "    ids, labels = [], []\n",
        "    with open(filename, 'r') as f:\n",
        "        line = f.readline();\n",
        "        while line:\n",
        "            line_split = line.split();\n",
        "\n",
        "            ids.append(line_split[0]);\n",
        "            labels.append(line_split[-1]);\n",
        "            line = f.readline();\n",
        "\n",
        "        return ids, labels\n",
        "\n",
        "\n",
        "'''\n",
        "根据互引用的关系构造有向图\n",
        "'''\n",
        "def load_graph(filename, id_list):\n",
        "    if directed:\n",
        "        g = nx.DiGraph()\n",
        "    else:\n",
        "        g = nx.Graph()\n",
        "    with open(filename, 'r') as f:\n",
        "        line = f.readline()\n",
        "        while line:\n",
        "            line_split = line.split()\n",
        "            #print(line_split)\n",
        "\n",
        "            if line_split[0] in id_list and line_split[1] in id_list  and line_split[0] != line_split[1]:\n",
        "                g.add_edge(line_split[0], line_split[1])\n",
        "                g[line_split[0]][line_split[1]]['weight'] = 1\n",
        "\n",
        "            line = f.readline()\n",
        "    return g\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_transition_probs(g, directed = False, p=1, q=1):\n",
        "    alias_nodes, alias_edges = {}, {};\n",
        "    for node in g.nodes():\n",
        "        probs = [g[node][nei]['weight'] for nei in sorted(g.neighbors(node))]\n",
        "        norm_const = sum(probs)\n",
        "        norm_probs = [float(prob)/norm_const for prob in probs]\n",
        "        alias_nodes[node] = get_alias_nodes(norm_probs)\n",
        "\n",
        "    if directed:\n",
        "        for edge in g.edges():\n",
        "            alias_edges[edge] = get_alias_edges(g, edge[0], edge[1], p, q)\n",
        "            #print(alias_edges[edge])\n",
        "    else:\n",
        "        for edge in g.edges():\n",
        "            alias_edges[edge] = get_alias_edges(g, edge[0], edge[1], p, q)\n",
        "            alias_edges[(edge[1], edge[0])] = get_alias_edges(g, edge[1], edge[0], p, q)\n",
        "\n",
        "    return alias_nodes, alias_edges\n",
        "\n",
        "\n",
        "def get_alias_edges(g, src, dest, p=1, q=1):\n",
        "    probs = [];\n",
        "    for nei in sorted(g.neighbors(dest)):\n",
        "        if nei==src:\n",
        "            probs.append(g[dest][nei]['weight']/p)\n",
        "        elif g.has_edge(nei, src):\n",
        "            probs.append(g[dest][nei]['weight'])\n",
        "        else:\n",
        "            probs.append(g[dest][nei]['weight']/q)\n",
        "    norm_probs = [float(prob)/sum(probs) for prob in probs]\n",
        "    return get_alias_nodes(norm_probs)\n",
        "\n",
        "'''\n",
        "针对节点t来说，我们得到了t能转移到不同类别节点的概率，\n",
        "常规做法是归一化之后按照概率随机选取，但这篇论文并没有直接这样做，而是选用了Alias算法进行抽样\n",
        "'''\n",
        "def get_alias_nodes(probs):\n",
        "    l = len(probs)\n",
        "    a, b = np.zeros(l), np.zeros(l, dtype=np.int64)\n",
        "    small, large = [], []\n",
        "\n",
        "    for i, prob in enumerate(probs):\n",
        "        a[i] = l*prob\n",
        "        if a[i]<1.0:\n",
        "            small.append(i)\n",
        "        else:\n",
        "            large.append(i)\n",
        "\n",
        "    while small and large:\n",
        "        sma, lar = small.pop(), large.pop()\n",
        "        b[sma] = lar\n",
        "        a[lar]+=a[sma]-1.0\n",
        "        if a[lar]<1.0:\n",
        "            small.append(lar)\n",
        "        else:\n",
        "            large.append(lar)\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def node2vec_walk(g, start, alias_nodes, alias_edges, walk_length=30):\n",
        "    path = [start]\n",
        "    while len(path)<walk_length:\n",
        "        node = path[-1]\n",
        "        neis = sorted(g.neighbors(node))\n",
        "        if len(neis)>0:\n",
        "            if len(path)==1:\n",
        "                l = len(alias_nodes[node][0])\n",
        "                idx = int(np.floor(np.random.rand()*l))\n",
        "                if np.random.rand()<alias_nodes[node][1][idx]:\n",
        "                    path.append(neis[idx])\n",
        "                else:\n",
        "                    path.append(neis[alias_nodes[node][0][idx]])\n",
        "            else:\n",
        "                prev = path[-2]\n",
        "                l = len(alias_edges[(prev, node)][0])\n",
        "                idx = int(np.floor(np.random.rand()*l))\n",
        "                if np.random.rand()<alias_edges[(prev, node)][1][idx]:\n",
        "                    path.append(neis[idx])\n",
        "                else:\n",
        "                    path.append(neis[alias_edges[(prev, node)][0][idx]])\n",
        "        else:\n",
        "            break\n",
        "    return path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "edge_path = 'cora.content'\n",
        "label_path = 'cora.cites'\n",
        "model_path = './output_node2vec.model'\n",
        "\n",
        "# load feature and adjacent matrix from file\n",
        "id_list, labels = load_features(edge_path)\n",
        "g = load_graph(label_path, id_list)#print(g)\n",
        "\n",
        "\n",
        "for node in id_list:\n",
        "    if not g.has_node(node):\n",
        "        g.add_node(node)\n",
        "\n",
        "\n",
        "if os.path.isfile(model_path):\n",
        "    model = Word2Vec.load(model_path)\n",
        "    print ('load model successfully')\n",
        "else:\n",
        "    alias_nodes, alias_edges = preprocess_transition_probs(g, directed,p,q)\n",
        "\n",
        "\n",
        "    walks = []\n",
        "    idx_total = []\n",
        "    for i in range(num_walks):\n",
        "        r = np.array(range(len(id_list)))\n",
        "        np.random.shuffle(r)\n",
        "        #r = list(r)\n",
        "        #idx_total+=r\n",
        "        for node in [id_list[j] for j in r]:\n",
        "            walks.append(node2vec_walk(g, node, alias_nodes, alias_edges, walk_length))\n",
        "\n",
        "    model = Word2Vec(walks, vector_size=emb_size, min_count=0, sg=1, epochs=iteration)\n",
        "    model.save('output_node2vec.model')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y=[]\n",
        "for temp in range(2708):\n",
        "    y.append( LABEL[labels[temp]])\n",
        "y = np.array(y)\n",
        "\n",
        "x_train = np.zeros(emb_size)\n",
        "x_test =  np.zeros(emb_size)\n",
        "\n",
        "\n",
        "droppoint = 500\n",
        "\n",
        "for x in range(droppoint):\n",
        "    x_train = np.row_stack((x_train,model.wv[id_list[x]]))\n",
        "x_train = np.delete(x_train,[0],axis = 0)\n",
        "y_train = y[:droppoint]\n",
        "\n",
        "\n",
        "for x in range(droppoint,1500):\n",
        "    x_test = np.row_stack((x_test,model.wv[id_list[x]]))\n",
        "x_test = np.delete(x_test,[0],axis = 0)\n",
        "y_test = y[droppoint:1500]\n",
        "\n",
        "#\n",
        "#neigh = ExtraTreesClassifier()\n",
        "#neigh.fit(x_train, y_train)\n",
        "#preds = neigh.predict(x_test)\n",
        "#print (list(preds-y_test).count(0)/500)\n",
        "\n",
        "\n",
        "classifier=LogisticRegression()\n",
        "classifier.fit(x_train,y_train)\n",
        "predictions=classifier.predict(x_test)\n",
        "print ('node2vec:')\n",
        "print (list(predictions-y_test).count(0)/1000)\n",
        "\n"
      ],
      "metadata": {
        "id": "JjOUUYJ-TkUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab7c37f-d55b-4ac2-c129-e51b03486961"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "node2vec:\n",
            "0.66\n"
          ]
        }
      ]
    }
  ]
}